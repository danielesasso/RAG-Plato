{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a2b984",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0400f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, pathlib\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# grammar/spell checker\n",
    "try:\n",
    "    import enchant\n",
    "    IT_DICT = enchant.Dict(\"it_IT\")          # Italian dictionary\n",
    "except Exception:\n",
    "    IT_DICT = None   # spell-error rate will be skipped if dict not present\n",
    "\n",
    "\n",
    "# ----------  sentence & word tokenizers ----------\n",
    "# 1) try native Italian; 2) English Punkt; 3) split\n",
    "def _build_sentence_splitter():\n",
    "    try:                                   \n",
    "        return nltk.data.load('tokenizers/punkt/italian.pickle').tokenize\n",
    "    except (LookupError, OSError):\n",
    "        try:                                \n",
    "            nltk.download('punkt', quiet=True)\n",
    "            return nltk.data.load('tokenizers/punkt/english.pickle').tokenize\n",
    "        except Exception:                   \n",
    "            return lambda txt: re.split(r'(?<=[.!?])\\s+', txt.strip())\n",
    "\n",
    "sent_split = _build_sentence_splitter()\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "def sentences(text):\n",
    "    return [s for s in sent_split(text) if s.strip()]\n",
    "\n",
    "def words(text):\n",
    "    return word_tokenizer.tokenize(text)\n",
    "\n",
    "# ----------  syllables (for Gulpease / F-V) ----------\n",
    "vowel_re = re.compile(r'[aeiouyàèéìòóù]+', re.I)\n",
    "def syllables(word:str):\n",
    "    return len(vowel_re.findall(word))\n",
    "\n",
    "# ----------  readability ----------\n",
    "def gulpease(text):\n",
    "    w = words(text); s = sentences(text)\n",
    "    if not w or not s: return None\n",
    "    letters = sum(len(wd) for wd in w)\n",
    "    return 89 + (300*len(s) - 10*letters) / len(w)\n",
    "\n",
    "def flesch_vacca(text):\n",
    "    w = words(text); s = sentences(text); syll = sum(syllables(wd) for wd in w)\n",
    "    if not w or not s: return None\n",
    "    return 206 - 1.3*(len(w)/len(s)) - 60*(syll/len(w))\n",
    "\n",
    "# ----------  writing quality ----------\n",
    "def spell_error_rate(text):\n",
    "    if IT_DICT is None: return None\n",
    "    tokens = [t for t in words(text) if t.isalpha()]\n",
    "    if not tokens: return None\n",
    "    errors = sum(not IT_DICT.check(t) for t in tokens)\n",
    "    return errors / len(tokens)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    s = sentences(text); w = words(text)\n",
    "    return len(w)/len(s) if s else None\n",
    "\n",
    "# ----------  heuristic named‑entity extraction (no spaCy) ----------\n",
    "_caps = re.compile(r'\\b[A-Z][A-Za-zÀ-ÖØ-Ýà-öø-ý]+\\b')  # capitalised words\n",
    "\n",
    "def _heuristic_entities(txt:str):\n",
    "    ents = []\n",
    "    for sent in sentences(txt):\n",
    "        for tok in words(sent):\n",
    "            if _caps.match(tok) and not tok.isupper():        # skip acronyms \n",
    "                ents.append(tok)\n",
    "    # also keep 4‑digit numbers (years, model names, etc.)\n",
    "    ents.extend(re.findall(r'\\b\\d{4}\\b', txt))\n",
    "    return set(ents)\n",
    "\n",
    "def entity_preservation_ratio(orig_en:str, summary_it:str):\n",
    "    ent_en = _heuristic_entities(orig_en)\n",
    "    ent_it = _heuristic_entities(summary_it)\n",
    "    return len(ent_en & ent_it) / len(ent_en) if ent_en else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d20ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file  gulpease  flesch_vacca spell_err  avg_sent_len\n",
      "RL.txt      43.9          32.0         —          19.8\n",
      "DL.txt      29.7          -7.8         —          42.3\n",
      "IR.txt      48.7          40.6         —          17.8\n",
      "ML.txt      45.7          31.1         —          16.1\n"
     ]
    }
   ],
   "source": [
    "orig_dir = pathlib.Path(\"originals\")\n",
    "sum_dir  = pathlib.Path(\"summaries\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for name in [\"RL.txt\", \"DL.txt\", \"IR.txt\", \"ML.txt\"]:\n",
    "    en = (orig_dir / name).read_text(encoding='utf-8')\n",
    "    it = (sum_dir  / name).read_text(encoding='utf-8')\n",
    "\n",
    "    rec = {\n",
    "        \"file\":          name,\n",
    "        # readability\n",
    "        \"gulpease\":      round(gulpease(it), 1),\n",
    "        \"flesch_vacca\":  round(flesch_vacca(it), 1),\n",
    "        # writing quality\n",
    "        \"spell_err\":     round(spell_error_rate(it), 3) if spell_error_rate(it) is not None else \"—\",\n",
    "        \"avg_sent_len\":  round(avg_sentence_length(it), 1),\n",
    "\n",
    "    }\n",
    "    records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
