{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fec10c",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a738df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def compute_bert_score(reference, candidate):\n",
    "    \"\"\"Returns precision, recall, and F1 based on BERT embeddings\"\"\"\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\")\n",
    "    return {\"bert_precision\": P.mean().item(),\n",
    "            \"bert_recall\": R.mean().item(),\n",
    "            \"bert_f1\": F1.mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27404cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "def compute_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)[0]\n",
    "    return {f\"rouge_{k}\": v for k,v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e70357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def compute_entailment_score(premise, hypothesis):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "    \n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Return probability of entailment (label 2 for MNLI)\n",
    "    return probabilities[0][2].item()\n",
    "\n",
    "def detect_hallucinations(source, summary):\n",
    "    \"\"\"Check each summary sentence against source\"\"\"\n",
    "    from nltk import sent_tokenize\n",
    "    source_sents = sent_tokenize(source)\n",
    "    summary_sents = sent_tokenize(summary)\n",
    "    \n",
    "    scores = []\n",
    "    for sent in summary_sents:\n",
    "        max_score = max(compute_entailment_score(source_sent, sent) \n",
    "                       for source_sent in source_sents)\n",
    "        scores.append(max_score)\n",
    "    \n",
    "    return {\n",
    "        \"hallucination_score\": 1 - (sum(scores)/len(scores)),\n",
    "        \"sentence_entailment\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd159d",
   "metadata": {},
   "source": [
    "## Gather summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee54aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 38, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel, SequenceSummary\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 49, in <module>\n",
      "    from .utils import (\n",
      "ImportError: cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/_9/92typr3d1_b38b6k16q2x3dr0000gn/T/ipykernel_77060/3225230901.py\", line 4, in <module>\n",
      "    from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1956, in __getattr__\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\n",
      "cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/miniconda3/envs/textsummary/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#––– 2. Perplexity –––\n",
    "# pip install transformers torch\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "def compute_perplexity(text: str, model_name: str = \"gpt2\"):\n",
    "    \"\"\"\n",
    "    Returns the perplexity of `text` under the specified GPT-2 model.\n",
    "    \"\"\"\n",
    "    # load tokenizer & model\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    model     = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize & run\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # outputs.loss is the average negative log‐likelihood per token\n",
    "        neg_log_likelihood = outputs.loss * input_ids.size(1)\n",
    "\n",
    "    ppl = torch.exp(outputs.loss)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589244a",
   "metadata": {},
   "source": [
    "## Surprise – quick latent-factor view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c010a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#––– 3. Flesch–Kincaid –––\n",
    "# pip install textstat\n",
    "import textstat\n",
    "\n",
    "def compute_readability(text: str):\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      - flesch_kincaid_grade: US grade level\n",
    "      - flesch_reading_ease:  higher = easier\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2b984",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca8680ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing DL.txt -> Deep Learning.txt\n",
      "\n",
      "🔍 Processing retrieval.txt -> Information Retrieval.txt\n",
      "\n",
      "🔍 Processing Reinforcement.txt -> Reinforcement Learning.txt\n",
      "\n",
      "🔍 Processing ML.txt -> Machine Learning.txt\n",
      "\n",
      "Final Results:\n",
      "\n",
      "DL.txt:\n",
      "\n",
      "retrieval.txt:\n",
      "\n",
      "Reinforcement.txt:\n",
      "\n",
      "ML.txt:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path  # More modern path handling\n",
    "\n",
    "def process_all_files():\n",
    "    # Get current directory (where evaluation.ipynb is)\n",
    "    root_dir = Path.cwd()\n",
    "    \n",
    "    # Define paths\n",
    "    originals_dir = root_dir / \"originals\"\n",
    "    summaries_dir = root_dir / \"summaries\"\n",
    "    \n",
    "    # Create mapping between summaries and originals\n",
    "    file_mapping = {\n",
    "        \"DL.txt\": \"Deep Learning.txt\",\n",
    "        \"retrieval.txt\": \"Information Retrieval.txt\",\n",
    "        \"Reinforcement.txt\": \"Reinforcement Learning.txt\",\n",
    "        \"ML.txt\": \"Machine Learning.txt\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for summary_name, original_name in file_mapping.items():\n",
    "        # Build full paths\n",
    "        original_path = originals_dir / original_name\n",
    "        summary_path = summaries_dir / summary_name\n",
    "        \n",
    "        # Check if files exist\n",
    "        if not original_path.exists():\n",
    "            print(f\"⚠️ Missing original: {original_path}\")\n",
    "            continue\n",
    "        if not summary_path.exists():\n",
    "            print(f\"⚠️ Missing summary: {summary_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Process the pair\n",
    "        print(f\"\\n🔍 Processing {summary_name} -> {original_name}\")\n",
    "        metrics = process_files(original_path, summary_path)\n",
    "        results[summary_name] = metrics\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Modified process_files to use pathlib\n",
    "def process_files(original_path, summary_path):\n",
    "    try:\n",
    "        with open(original_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {original_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            summary = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {summary_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "    # ... your existing metric calculations ...\n",
    "    return metrics\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = process_all_files()\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for name, metrics in all_results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k:>25}: {v:.4f}\" if isinstance(v, float) else f\"{k:>25}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0400f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, pathlib\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# optional grammar/spell checker\n",
    "try:\n",
    "    import enchant\n",
    "    IT_DICT = enchant.Dict(\"it_IT\")          # Italian dictionary\n",
    "except Exception:\n",
    "    IT_DICT = None   # spell-error rate will be skipped if dict not present\n",
    "\n",
    "\n",
    "# ----------  sentence & word tokenizers ----------\n",
    "# 1) try native Italian Punkt; 2) fall back to English Punkt; 3) regex split\n",
    "def _build_sentence_splitter():\n",
    "    try:                                    # 1️⃣ Italian model (rarely available now)\n",
    "        return nltk.data.load('tokenizers/punkt/italian.pickle').tokenize\n",
    "    except (LookupError, OSError):\n",
    "        try:                                # 2️⃣ Generic English Punkt\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            return nltk.data.load('tokenizers/punkt/english.pickle').tokenize\n",
    "        except Exception:                   # 3️⃣ Dumb regex fallback\n",
    "            return lambda txt: re.split(r'(?<=[.!?])\\s+', txt.strip())\n",
    "\n",
    "sent_split = _build_sentence_splitter()\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "def sentences(text):\n",
    "    return [s for s in sent_split(text) if s.strip()]\n",
    "\n",
    "def words(text):\n",
    "    return word_tokenizer.tokenize(text)\n",
    "\n",
    "# ----------  syllables (for Gulpease / F-V) ----------\n",
    "vowel_re = re.compile(r'[aeiouyàèéìòóù]+', re.I)\n",
    "def syllables(word:str):\n",
    "    return len(vowel_re.findall(word))\n",
    "\n",
    "# ----------  readability ----------\n",
    "def gulpease(text):\n",
    "    w = words(text); s = sentences(text)\n",
    "    if not w or not s: return None\n",
    "    letters = sum(len(wd) for wd in w)\n",
    "    return 89 + (300*len(s) - 10*letters) / len(w)\n",
    "\n",
    "def flesch_vacca(text):\n",
    "    w = words(text); s = sentences(text); syll = sum(syllables(wd) for wd in w)\n",
    "    if not w or not s: return None\n",
    "    return 206 - 1.3*(len(w)/len(s)) - 60*(syll/len(w))\n",
    "\n",
    "# ----------  writing quality ----------\n",
    "def spell_error_rate(text):\n",
    "    if IT_DICT is None: return None\n",
    "    tokens = [t for t in words(text) if t.isalpha()]\n",
    "    if not tokens: return None\n",
    "    errors = sum(not IT_DICT.check(t) for t in tokens)\n",
    "    return errors / len(tokens)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    s = sentences(text); w = words(text)\n",
    "    return len(w)/len(s) if s else None\n",
    "\n",
    "# ----------  hallucination / term faithfulness ----------\n",
    "tech_term_re = re.compile(r'[A-Z]{2,}[A-Za-z]*')   # crude heuristic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d20ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file  gulpease  flesch_vacca spell_err  avg_sent_len  term_ratio  jaccard_3gram\n",
      "RL.txt      43.9          32.0         —          19.8       0.000          0.000\n",
      "DL.txt      29.7          -7.8         —          42.3       0.032          0.001\n",
      "IR.txt      48.7          40.6         —          17.8       0.100          0.000\n",
      "ML.txt      45.7          31.1         —          16.1       0.048          0.000\n"
     ]
    }
   ],
   "source": [
    "orig_dir = pathlib.Path(\"originals\")\n",
    "sum_dir  = pathlib.Path(\"summaries\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for name in [\"RL.txt\", \"DL.txt\", \"IR.txt\", \"ML.txt\"]:\n",
    "    en = (orig_dir / name).read_text(encoding='utf-8')\n",
    "    it = (sum_dir  / name).read_text(encoding='utf-8')\n",
    "\n",
    "    rec = {\n",
    "        \"file\":          name,\n",
    "        # readability\n",
    "        \"gulpease\":      round(gulpease(it), 1),\n",
    "        \"flesch_vacca\":  round(flesch_vacca(it), 1),\n",
    "        # writing quality\n",
    "        \"spell_err\":     round(spell_error_rate(it), 3) if spell_error_rate(it) is not None else \"—\",\n",
    "        \"avg_sent_len\":  round(avg_sentence_length(it), 1),\n",
    "    }\n",
    "    records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
