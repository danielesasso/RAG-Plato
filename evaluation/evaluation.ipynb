{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fec10c",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a738df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#––– 1. BERTScore –––\n",
    "# pip install bert-score\n",
    "from bert_score import score\n",
    "\n",
    "def compute_bertscore(source: str, summary: str, lang: str = \"en\"):\n",
    "    \"\"\"\n",
    "    Returns (precision, recall, F1) of summary vs. source.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(\n",
    "        [summary],      # candidate summaries\n",
    "        [source],       # references (here: the original text)\n",
    "        lang=lang,\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": P.mean().item(),\n",
    "        \"recall\":    R.mean().item(),\n",
    "        \"f1\":        F1.mean().item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd159d",
   "metadata": {},
   "source": [
    "## Gather summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee54aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#––– 2. Perplexity –––\n",
    "# pip install transformers torch\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "def compute_perplexity(text: str, model_name: str = \"gpt2\"):\n",
    "    \"\"\"\n",
    "    Returns the perplexity of `text` under the specified GPT-2 model.\n",
    "    \"\"\"\n",
    "    # load tokenizer & model\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    model     = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize & run\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # outputs.loss is the average negative log‐likelihood per token\n",
    "        neg_log_likelihood = outputs.loss * input_ids.size(1)\n",
    "\n",
    "    ppl = torch.exp(outputs.loss)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589244a",
   "metadata": {},
   "source": [
    "## Surprise – quick latent-factor view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c010a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#––– 3. Flesch–Kincaid –––\n",
    "# pip install textstat\n",
    "import textstat\n",
    "\n",
    "def compute_readability(text: str):\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      - flesch_kincaid_grade: US grade level\n",
    "      - flesch_reading_ease:  higher = easier\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2b984",
   "metadata": {},
   "source": [
    "## RecBole – needs its own folder layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8680ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DL.txt\n",
      "\n",
      "Perplexity → 106.84957885742188\n",
      "\n",
      "Readability → {'flesch_kincaid_grade': 25.1, 'flesch_reading_ease': -12.15}\n",
      "----------------------------------------\n",
      "\n",
      "retrieval.txt\n",
      "\n",
      "Perplexity → 213.31503295898438\n",
      "\n",
      "Readability → {'flesch_kincaid_grade': 14.6, 'flesch_reading_ease': 20.38}\n",
      "----------------------------------------\n",
      "\n",
      "Reinforcement.txt\n",
      "\n",
      "Perplexity → 78.48356628417969\n",
      "\n",
      "Readability → {'flesch_kincaid_grade': 16.5, 'flesch_reading_ease': 10.19}\n",
      "----------------------------------------\n",
      "\n",
      "ML.txt\n",
      "\n",
      "Perplexity → 129.42745971679688\n",
      "\n",
      "Readability → {'flesch_kincaid_grade': 13.1, 'flesch_reading_ease': 29.86}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_files(file_list):\n",
    "    for fname in file_list:\n",
    "        try:\n",
    "            with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "                source = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\" File non trovato: {fname}\")\n",
    "            continue\n",
    "\n",
    "        ppl = compute_perplexity(source)\n",
    "        rd  = compute_readability(source)\n",
    "\n",
    "        print(f\"\\n{fname}\")\n",
    "        print(f\"\\nPerplexity → {ppl}\")\n",
    "        print(f\"\\nReadability → {rd}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = [\"DL.txt\", \"retrieval.txt\", \"Reinforcement.txt\", \"ML.txt\"]\n",
    "    process_files(files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
